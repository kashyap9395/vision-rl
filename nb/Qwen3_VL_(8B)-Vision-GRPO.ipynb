{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfg1EnBO7tzQ"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://unsloth.ai/docs/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://unsloth.ai/docs/get-started/install).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & how to save it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Y8w2_x7tzd"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJL9onBV7tzi"
      },
      "source": [
        "Long-Context GRPO for reinforcement learning ‚Äî train stably at massive sequence lengths. Fine-tune models with up to 7x more context length efficiently. [Read Blog](https://unsloth.ai/docs/new/grpo-long-context)\n",
        "\n",
        "3√ó faster training with optimized sequence packing ‚Äî higher throughput with no quality loss.[Read Blog](https://unsloth.ai/docs/new/3x-faster-training-packing)\n",
        "\n",
        "500k context-length fine-tuning ‚Äî push long-context models further with memory-efficient training. [Read Blog](https://unsloth.ai/docs/new/500k-context-length-fine-tuning)\n",
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://unsloth.ai/docs/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://unsloth.ai/docs/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://unsloth.ai/docs/get-started/all-our-models) and [notebooks](https://unsloth.ai/docs/get-started/unsloth-notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FAt_r1w7tzl"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QVSo3mHG7tzl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.26.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIy3QkjW1O4R"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B3HIT0t6nc0"
      },
      "source": [
        "We're also introducing how you can do `GSPO` inside of Unsloth as well!\n",
        "\n",
        "The goal of this notebook is to make a vision language model solve maths problems via reinforcement learning given an image input like below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/lupantech/MathVista/main/assets/our_new_3_datasets.png\" alt=\"Alt text\" height=\"256\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cI_m3OJWja8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "45e0912765f94dba86dc8147ea096d67",
            "6a592b4521974538b82c99598bf10e53",
            "2da5031646354e8992499f2604330a1e",
            "d5c79cbc58f14130a2d70a22b7f5b23d",
            "aa94704ee5d5437cb742a5220eb79b97",
            "a17c0c25ff9046c39282fe10cad2d963",
            "afc5cc9d9e1f40c4924aed631323d9a8",
            "53164aa110be43e4ae4d95ee7d1140a7",
            "39e6abad472049ea994876f8a23fbae6",
            "2acf9c10af41460fb936320e08455984",
            "20e0b5595e464fd2bb050ab8b618224c"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "3f6e7c8c-8e99-4983-c6b7-8d7055046ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.4: Fast Qwen3_Vl patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45e0912765f94dba86dc8147ea096d67"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "max_seq_length = 16384 # Must be this long for VLMs\n",
        "lora_rank = 16 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = False, # Enable vLLM fast inference\n",
        "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOXrl8iLQx6S"
      },
      "source": [
        "In Unsloth, we share vLLM's weights directly, reducing VRAM usage by > 50%. vLLM also does not yet support LoRA on the vision layers, so we can only add them on the language layers. Vision GRPO still works though!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pmZ6zZ5AQu7I"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True,  # False if not finetuning language layers\n",
        "    finetune_attention_modules = True,  # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True,  # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "`AI4Math/MathVista` is a dataset that involves using images to solve logic and math problems.\n",
        "\n",
        "For this notebook, we will only use math problems with numeric answers for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7zM1VPx5KcpF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "dataset = load_dataset(\"AI4Math/MathVista\", split = \"testmini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0CeDQrm6BWW"
      },
      "source": [
        "We filter the dataset to keep only float or numeric answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iw8EVJmp5rsC"
      },
      "outputs": [],
      "source": [
        "def is_numeric_answer(example):\n",
        "    try:\n",
        "        float(example[\"answer\"])\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "dataset = dataset.filter(is_numeric_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAPtJzy_5uLh"
      },
      "source": [
        "We also resize the images to be 512 by 512 pixels to make the images managable in context length. We also convert them to RGB so they are compatible for training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tT8WNP1A5tsh"
      },
      "outputs": [],
      "source": [
        "# Resize to (512, 512)\n",
        "def resize_images(example):\n",
        "    image = example[\"decoded_image\"]\n",
        "    image = image.resize((512, 512))\n",
        "    example[\"decoded_image\"] = image\n",
        "    return example\n",
        "dataset = dataset.map(resize_images)\n",
        "\n",
        "# Then convert to RGB\n",
        "def convert_to_rgb(example):\n",
        "    image = example[\"decoded_image\"]\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    example[\"decoded_image\"] = image\n",
        "    return example\n",
        "dataset = dataset.map(convert_to_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2WpGKjZ7mHI"
      },
      "source": [
        "We then create the conversational template that is needed to collate the dataset for RL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-lvgcXGjk_a6"
      },
      "outputs": [],
      "source": [
        "# Define the delimiter variables for clarity and easy modification\n",
        "REASONING_START = \"<REASONING>\"\n",
        "REASONING_END = \"</REASONING>\"\n",
        "SOLUTION_START = \"<SOLUTION>\"\n",
        "SOLUTION_END = \"</SOLUTION>\"\n",
        "\n",
        "def make_conversation(example):\n",
        "    # Define placeholder constants if they are not defined globally\n",
        "    # The user's text prompt\n",
        "    text_content = (\n",
        "        f\"{example['question']}. Also first provide your reasoning or working out\"\\\n",
        "        f\" on how you would go about solving the question between {REASONING_START} and {REASONING_END}\"\n",
        "        f\" and then your final answer between {SOLUTION_START} and (put a single float here) {SOLUTION_END}\"\n",
        "    )\n",
        "\n",
        "    # Construct the prompt in the desired multi-modal format\n",
        "    prompt = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},  # Placeholder for the image\n",
        "                {\"type\": \"text\", \"text\": text_content},  # The text part of the prompt\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    # The actual image data is kept separate for the processor\n",
        "    return {\"prompt\": prompt, \"image\": example[\"decoded_image\"], \"answer\": example[\"answer\"]}\n",
        "\n",
        "train_dataset = dataset.map(make_conversation)\n",
        "\n",
        "# We're reformatting dataset like this because decoded_images are the actual images\n",
        "# The \"image\": example[\"decoded_image\"] does not properly format the dataset correctly\n",
        "\n",
        "# 1. Remove the original 'image' column\n",
        "train_dataset = train_dataset.remove_columns(\"image\")\n",
        "\n",
        "# 2. Rename 'decoded_image' to 'image'\n",
        "train_dataset = train_dataset.rename_column(\"decoded_image\", \"image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOz-lAoI5fLW"
      },
      "source": [
        "Now let's apply the chat template across the entire dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs2HiThleic"
      },
      "source": [
        "## Reward functions\n",
        "\n",
        "We now define some basic formatting rewards functions to see if reasoning starts and ends, and also another to see if the answers were written correctly.\n",
        "\n",
        "We also try to fix the `addCriterion` issue as described in our [blog post](https://unsloth.ai/docs/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "# Reward functions\n",
        "import re\n",
        "\n",
        "def formatting_reward_func(completions,**kwargs):\n",
        "    import re\n",
        "    thinking_pattern = f'{REASONING_START}(.*?){REASONING_END}'\n",
        "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
        "\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        if isinstance(completion, list):\n",
        "            completion = completion[0][\"content\"] if completion else \"\"\n",
        "        score = 0\n",
        "        thinking_matches = re.findall(thinking_pattern, completion, re.DOTALL)\n",
        "        answer_matches = re.findall(answer_pattern, completion, re.DOTALL)\n",
        "        if len(thinking_matches) == 1:\n",
        "            score += 1.0\n",
        "        if len(answer_matches) == 1:\n",
        "            score += 1.0\n",
        "\n",
        "        # Fix up addCriterion issues\n",
        "        # See https://unsloth.ai/docs/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks\n",
        "        # Penalize on excessive addCriterion and newlines\n",
        "        if len(completion) != 0:\n",
        "            removal = completion.replace(\"addCriterion\", \"\").replace(\"\\n\", \"\")\n",
        "            if (len(completion)-len(removal))/len(completion) >= 0.5:\n",
        "                score -= 2.0\n",
        "\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
        "\n",
        "    completions = [(c[0][\"content\"] if c else \"\") if isinstance(c, list) else c for c in completions]\n",
        "    responses = [re.findall(answer_pattern, completion, re.DOTALL) for completion in completions]\n",
        "    q = prompts[0]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:{completions[0]}\")\n",
        "    return [\n",
        "        2.0 if len(r)==1 and a == r[0].replace('\\n','') else 0.0\n",
        "        for r, a in zip(responses, answer)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrst4oDS5AFO"
      },
      "source": [
        "Here is the first example prompt in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXjAu-nQwpuI",
        "outputId": "eec20388-f45c-4821-e4ee-7eea4303371e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': [{'text': None, 'type': 'image'},\n",
              "   {'text': \"When a spring does work on an object, we cannot find the work by simply multiplying the spring force by the object's displacement. The reason is that there is no one value for the force-it changes. However, we can split the displacement up into an infinite number of tiny parts and then approximate the force in each as being constant. Integration sums the work done in all those parts. Here we use the generic result of the integration.\\r\\n\\r\\nIn Figure, a cumin canister of mass $m=0.40 \\\\mathrm{~kg}$ slides across a horizontal frictionless counter with speed $v=0.50 \\\\mathrm{~m} / \\\\mathrm{s}$. It then runs into and compresses a spring of spring constant $k=750 \\\\mathrm{~N} / \\\\mathrm{m}$. When the canister is momentarily stopped by the spring, by what distance $d$ is the spring compressed?. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION>\",\n",
              "    'type': 'text'}],\n",
              "  'role': 'user'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKBOl4K1wpWx",
        "outputId": "d4e61b4e-6976-418a-89b0-0592409a0b8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': [{'text': None, 'type': 'image'},\n",
              "   {'text': 'Move the ruler to measure the length of the nail to the nearest inch. The nail is about (_) inches long.. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION>',\n",
              "    'type': 'text'}],\n",
              "  'role': 'user'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_dataset[100][\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YpenSUIAczo"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model on the hundredth sample of the train dataset without training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = train_dataset[100][\"image\"]\n",
        "prompt = train_dataset[100][\"prompt\"]\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfkBQLaQAyw3",
        "outputId": "cb63d4f1-9ff0-47ca-a0a1-3441cfe90511"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': [{'text': None, 'type': 'image'},\n",
              "   {'text': 'Move the ruler to measure the length of the nail to the nearest inch. The nail is about (_) inches long.. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION>',\n",
              "    'type': 'text'}],\n",
              "  'role': 'user'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGa0rueD5Mfh",
        "outputId": "0de3a869-86e0-4446-f374-85adc60af20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<REASONING>\n",
            "To measure the length of the nail to the nearest inch, I need to determine how long the nail is by comparing it to the ruler.\n",
            "\n",
            "Step 1: Identify the starting point of the nail.\n",
            "The head of the nail (the circular part) is aligned with the 0-inch mark on the ruler.\n",
            "\n",
            "Step 2: Identify the ending point of the nail.\n",
            "The sharp tip of the nail extends past the 3-inch mark but does not reach the 4-inch mark.\n",
            "\n",
            "Step 3: Determine the length to the nearest inch.\n",
            "Since the tip is between 3 and 4 inches, and we are rounding to the nearest inch, I need to see which whole number it is closer to. The tip is clearly past 3 and before 4, so I need to estimate if it's closer to 3 or 4.\n",
            "\n",
            "Looking at the visual: the tip appears to be just shy of the 4-inch mark. It‚Äôs about 3.5 inches long (since it‚Äôs roughly halfway between 3 and 4), but since we need to round to the nearest inch, 3.5 rounds up to 4. However, let‚Äôs be precise. The nail's tip looks to be at approximately 3.2 to 3.4 inches. Since this is less than 3.5, it rounds down to 3 inches.\n",
            "\n",
            "But wait ‚Äî let‚Äôs check again. The question says \"to the nearest inch\". The nail‚Äôs tip is clearly past 3 and is very close to 4, but not quite there. If it were exactly 3.5 inches, it would round to 4. But visually, it‚Äôs not 3.5 ‚Äî it‚Äôs less than that. It appears to be about 3.3 inches. 3.3 is closer to 3 than to 4, so it rounds to 3.\n",
            "\n",
            "However, looking at the image more carefully, the tip appears to be just barely past the 3-inch mark and seems to be very close to the 4-inch mark ‚Äî perhaps even a bit over 3.5. But since we are rounding to the nearest inch, we look at the midpoint between 3 and 4, which is 3.5. If the nail‚Äôs length is 3.5 inches or more, we round up to 4. If it‚Äôs less than 3.5, we round down to 3.\n",
            "\n",
            "In the image, the nail‚Äôs tip appears to be past 3.5. Let me estimate: from 3 to 4, the ruler is marked in 0.5 inch increments (since each major tick is 1 inch, and the midpoints are marked implicitly). The tip appears to be just before the 4-inch mark. So if it‚Äôs at 3.8 inches, that‚Äôs closer to 4 than to 3, so it rounds to 4. But if it‚Äôs at 3.2, it rounds to 3.\n",
            "\n",
            "Looking at the image, the tip is past the 3-inch mark and appears to be just shy of the 4-inch mark ‚Äî maybe 3.7 or 3.8 inches. Since 3.7 and 3.8 are both greater than 3.5, they round up to 4.\n",
            "\n",
            "Therefore, to the nearest inch, the nail is 4 inches long.\n",
            "\n",
            "But wait ‚Äî let‚Äôs be precise. Let me imagine the ruler: 0, 1, 2, 3, 4, 5, 6. The nail‚Äôs head is at 0. The tip is clearly beyond 3 and appears to be very close to 4, but not quite there. Is it 3.5? No, it‚Äôs less than 3.5? No ‚Äî if it were 3.5, it would be exactly halfway. Since it‚Äôs visually past 3.5 (because it‚Äôs not at 3.5, and it‚Äôs getting close to 4), it should round to 4.\n",
            "\n",
            "Actually, I need to be careful. Let me measure it. From 0 to 3 is 3 inches. The tip is past 3, so it‚Äôs more than 3. If it were 3.1, it would round to 3. If it were 3.6, it would round to 4. The nail in the image appears to be approximately 3.6 to 3.7 inches long. Since 3.6 > 3.5, it rounds to 4.\n",
            "\n",
            "So, to the nearest inch, the nail is 4 inches long.\n",
            "\n",
            "</REASONING>\n",
            "<SOLUTION>4.0</SOLUTION><|im_end|>\n"
          ]
        }
      ],
      "source": [
        "image = train_dataset[100][\"image\"]\n",
        "structured_prompt = train_dataset[100][\"prompt\"]\n",
        "\n",
        "# 1. Convert the structured prompt (list of dictionaries) into a single string\n",
        "#    This will apply the chat template and embed image tokens (e.g., <|vision_start|>) into the string.\n",
        "formatted_text_with_image_tokens = tokenizer.apply_chat_template(\n",
        "    structured_prompt,\n",
        "    tokenize=False, # We want the raw string output from the template\n",
        "    add_generation_prompt=True, # Add the assistant turn start token if necessary\n",
        "    return_tensors=None, # Ensure a string is returned\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    formatted_text_with_image_tokens, # Pass the formatted string containing image tokens\n",
        "    add_special_tokens = False, # Let apply_chat_template handle special tokens\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
        "                   use_cache = True, temperature = 1.0, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up the `GRPO` Trainer and all configurations! Note we actually enable `GSPO` as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "dc195e1f-ff50-412d-8cbf-971c5545ad4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 2\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    log_completions = False,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 2, # Decrease if out of memory\n",
        "    max_prompt_length = 1024,\n",
        "    max_completion_length = 1024,\n",
        "    num_train_epochs = 0.5, # Set to 1 for a full training run\n",
        "    # max_steps = 60,\n",
        "    save_steps = 60,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # Below enables GSPO:\n",
        "    importance_sampling_level = \"sequence\",\n",
        "    mask_truncated_completions = False,\n",
        "    loss_type = \"dr_grpo\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n",
        "\n",
        "During inference, you might encounter `addCriterion` or some weird gibberish outputs. Please read our [blog post](https://unsloth.ai/docs/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks) on why this occurs. It seems to be an inherent thing inside of the model, and we can ignore this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "6fUaoYJEKgpb",
        "outputId": "197e4910-780a-426d-9fa7-9f6928968f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 566 | Num Epochs = 1 | Total steps = 283\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 43,646,976 of 8,810,770,672 (0.50% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "[{'content': [{'text': None, 'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x7B5D87628C20>}, {'text': 'What is the highest value on the X axis?. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION>', 'type': 'text'}], 'role': 'user'}] \n",
            "Answer:\n",
            "30 \n",
            "Response:<REASONING>\n",
            "To determine the highest value on the X-axis of the graph, I examine the horizontal axis (X-axis) which is labeled \"MICROGRAMS/ml-E-DNP-LYSINE-HCL\". I observe the scale markings on this axis, which are marked at intervals of 5 units: 0, 5, 10, 15, 20, 25, and 30. The last marked value on the X-axis is 30. Since the axis extends to this point and there are no further labeled tick marks beyond 30, the highest value indicated on the X-axis is 30.\n",
            "</REASONING>\n",
            "<SOLUTION>30.0</SOLUTION>\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2329777489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Restore previous mode when possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   3006\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2944\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2946\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_dtype_grads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     torch._amp_foreach_non_finite_check_and_unscale_(\n\u001b[0m\u001b[1;32m    281\u001b[0m                         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0mper_device_found_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    # Pass the processor to handle multimodal inputs\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        formatting_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    train_dataset = train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzUvkjO6ffIs"
      },
      "source": [
        "Let's run the model! You can modify the instruction and input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtcz_lpbVC92",
        "outputId": "7dab8bf6-22c3-4ec1-a4e6-2a2af91ce3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<REASONING>\n",
            "To find the magnitude of the average force on the driver during the collision, we need to apply the impulse-momentum theorem. The impulse (change in momentum) equals the average force multiplied by the time interval over which the force acts.\n",
            "\n",
            "The change in momentum is given by:\n",
            "Œîp = m * Œîv = m * (v_f - v_i)\n",
            "\n",
            "However, since the velocities are vectors (with directions), we must resolve them into components perpendicular and parallel to the wall, because the wall exerts a force only perpendicular to the wall (normal force), and the parallel component of velocity typically does not change in an ideal elastic collision (though here it's inelastic, the parallel component might still be unchanged if the wall is perfectly rigid and the collision is only normal).\n",
            "\n",
            "In this case, the car‚Äôs velocity has components parallel and perpendicular to the wall. The wall only exerts a force perpendicular to itself. Therefore, the component of velocity parallel to the wall should remain unchanged, and only the component perpendicular to the wall changes.\n",
            "\n",
            "Let‚Äôs define:\n",
            "- The wall is along the x-axis (as shown in the diagram, with the wall horizontal, and y-axis perpendicular to it).\n",
            "- The car‚Äôs initial velocity vector makes an angle of 30¬∞ with the wall, so the angle with the y-axis (perpendicular to wall) is 60¬∞, but we can directly use the angle given with respect to the wall.\n",
            "\n",
            "Actually, looking at the diagram:\n",
            "- The initial velocity is at 30¬∞ from the wall (so 30¬∞ from the x-axis).\n",
            "- The final velocity is at 10¬∞ from the wall (so 10¬∞ from the x-axis).\n",
            "\n",
            "Therefore:\n",
            "- The component of velocity parallel to the wall (x-direction) is: v_x = v * cos(Œ∏)\n",
            "- The component of velocity perpendicular to the wall (y-direction) is: v_y = v * sin(Œ∏)\n",
            "\n",
            "The wall exerts a force only in the y-direction (perpendicular to the wall). Therefore, the change in momentum in the y-direction will be what matters for the average force, because the x-component doesn't change (no force in x-direction from the wall).\n",
            "\n",
            "So, let's compute the initial and final y-components of velocity:\n",
            "\n",
            "Initial velocity:\n",
            "v_i = 70 m/s at 30¬∞ from the wall (x-axis)\n",
            "So, initial y-component: v_{iy} = 70 * sin(30¬∞) = 70 * 0.5 = 35 m/s (in the positive y-direction, since the car is approaching the wall from below, and the y-axis is upward)\n",
            "\n",
            "Final velocity:\n",
            "v_f = 50 m/s at 10¬∞ from the wall (x-axis)\n",
            "So, final y-component: v_{fy} = 50 * sin(10¬∞) ‚âà 50 * 0.173648 ‚âà 8.6824 m/s (but note: after collision, the car is moving away from the wall? Or is it still approaching? The diagram shows the car moving away from the wall after collision, since it's shown going away from the wall. But if the car is colliding with the wall and then continues, the y-component should be negative after collision? Wait, let's think.\n",
            "\n",
            "Looking at the diagram:\n",
            "- The car is approaching the wall from below (the arrow points toward the wall from the lower left, at 30¬∞ to the x-axis). So, the y-component is downward, meaning negative if we take upward as positive y.\n",
            "- After collision, the car is moving away from the wall, to the right and slightly upward (10¬∞ from the x-axis). So the y-component is upward, positive.\n",
            "\n",
            "But the initial y-component is downward (toward the wall), so it should be negative. Let's define the y-axis as positive upward. Then:\n",
            "\n",
            "Initial y-component: v_{iy} = -70 * sin(30¬∞) = -35 m/s\n",
            "Final y-component: v_{fy} = +50 * sin(10¬∞) ‚âà +8.6824 m/s\n",
            "\n",
            "Therefore, the change in y-component of velocity: Œîv_y = v_{fy} - v_{iy} = 8.6824 - (-35) = 8.6824 + 35 = 43.6824 m/s\n",
            "\n",
            "Then, change in momentum in y-direction: Œîp_y = m * Œîv_y = 80 * 43.6824 = 3494.592 kg¬∑m/s\n",
            "\n",
            "The time interval of collision: Œît = 14 ms = 0.014 s\n",
            "\n",
            "Average force in y-direction: F_avg = Œîp_y / Œît = 3494.\n"
          ]
        }
      ],
      "source": [
        "image = train_dataset[165][\"image\"]\n",
        "prompt = train_dataset[165][\"prompt\"]\n",
        "\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    prompt,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
        "                   use_cache = True, temperature = 1.0, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, use Hugging Face‚Äôs `push_to_hub` for online saving, or `save_pretrained` for local storage.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL-BcuB1VLIv",
        "outputId": "905cc0c8-ebb9-4e2a-8b13-db77c054c222"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"grpo_lora\")  # Local saving\n",
        "tokenizer.save_pretrained(\"grpo_lora\")\n",
        "# model.push_to_hub(\"your_name/grpo_lora\", token = \"...\") # Online saving\n",
        "# processor.push_to_hub(\"your_name/grpo_lora\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07lMVV96vz39"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbm6rx-dbt5Z"
      },
      "source": [
        "Special Credits to [GAD-Cell](https://github.com/GAD-cell) for helping Unsloth create this notebook and bringing VLM GRPO into Unsloth!\n",
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other resources:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "4. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://unsloth.ai/docs/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://unsloth.ai/docs/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45e0912765f94dba86dc8147ea096d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a592b4521974538b82c99598bf10e53",
              "IPY_MODEL_2da5031646354e8992499f2604330a1e",
              "IPY_MODEL_d5c79cbc58f14130a2d70a22b7f5b23d"
            ],
            "layout": "IPY_MODEL_aa94704ee5d5437cb742a5220eb79b97"
          }
        },
        "6a592b4521974538b82c99598bf10e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a17c0c25ff9046c39282fe10cad2d963",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_afc5cc9d9e1f40c4924aed631323d9a8",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "2da5031646354e8992499f2604330a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53164aa110be43e4ae4d95ee7d1140a7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39e6abad472049ea994876f8a23fbae6",
            "value": 2
          }
        },
        "d5c79cbc58f14130a2d70a22b7f5b23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2acf9c10af41460fb936320e08455984",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20e0b5595e464fd2bb050ab8b618224c",
            "value": "‚Äá2/2‚Äá[00:31&lt;00:00,‚Äá14.90s/it]"
          }
        },
        "aa94704ee5d5437cb742a5220eb79b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17c0c25ff9046c39282fe10cad2d963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc5cc9d9e1f40c4924aed631323d9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53164aa110be43e4ae4d95ee7d1140a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e6abad472049ea994876f8a23fbae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2acf9c10af41460fb936320e08455984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e0b5595e464fd2bb050ab8b618224c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}